{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://spark-master-hostname.spark-master-headless.default.svc.cluster.local:4040\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = spark://spark-master:7077, app id = app-20200823030147-0008)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.Pipeline\n",
       "import org.apache.spark.ml.feature.{CountVectorizer, RegexTokenizer, StopWordsRemover, VectorAssembler}\n",
       "import org.apache.spark.sql.{DataFrame, SparkSession}\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.feature.{CountVectorizer, RegexTokenizer, StopWordsRemover, VectorAssembler}\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@78da443a\n",
       "sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@791ca960\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "      .builder()\n",
    "      .appName(\"Malware_Feature_Engineering_2\")\n",
    "      .getOrCreate()\n",
    "\n",
    "val sc=spark.sparkContext\n",
    "\n",
    "sc.hadoopConfiguration.set(\"fs.s3a.endpoint\", \"https://s3.amazonaws.com\")\n",
    "sc.hadoopConfiguration.set(\"fs.s3a.access.key\", \"\")\n",
    "sc.hadoopConfiguration.set(\"fs.s3a.secret.key\", \"+Vs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n",
       "import spark.sql\n",
       "df_1: org.apache.spark.sql.DataFrame = [file_name: string, File: string ... 2 more fields]\n",
       "res1: df_1.type = [file_name: string, File: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "import spark.sql\n",
    "\n",
    "val df_1 = spark.read.format(\"parquet\").option(\"inferSchema\", \"true\").\n",
    "    option(\"header\", \"false\").\n",
    "    load(\"s3a://ml-workflow-data/security/Malware_Dataset/malware_tmp_parquet_1\")\n",
    "\n",
    "df_1.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "featureEngineeringCountV: (dataMatrix: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def featureEngineeringCountV(dataMatrix: DataFrame): DataFrame ={\n",
    "\n",
    "      val tokenizer = new RegexTokenizer().setInputCol(\"Text\").setOutputCol(\"wordsArray\")\n",
    "      val remover = new StopWordsRemover().setInputCol(\"wordsArray\").setOutputCol(\"filteredWords\")\n",
    "      val cnt = new  CountVectorizer().setInputCol(\"filteredWords\").setOutputCol(\"countv\").setVocabSize(20)\n",
    "      val assembler = new VectorAssembler().setInputCols(Array(\"countv\")).setOutputCol(\"features\")\n",
    "      val pipeline = new Pipeline().setStages(Array(tokenizer, remover, cnt, assembler))\n",
    "      val model= pipeline.fit(dataMatrix)\n",
    "      model.transform(dataMatrix).drop(\"RawText\",\"wordsArray\",\"filteredWords\")\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Int = 32\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val final_df = featureEngineeringCountV(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.write.mode(\"overwrite\").format(\"parquet\").option(\"compression\", \"snappy\").mode(\"overwrite\").save(\"s3a://ml-workflow-data/security/Malware_Dataset/malware_tmp_parquet_2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}